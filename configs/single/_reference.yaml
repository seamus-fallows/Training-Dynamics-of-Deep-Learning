# =============================================================================
# SINGLE EXPERIMENT CONFIG REFERENCE
# =============================================================================
# This file documents all available config fields. Don't run this directly —
# copy and modify for your experiment.
# =============================================================================

experiment:
  name: my_experiment  # string, used for output directory naming

# -----------------------------------------------------------------------------
# Model
# -----------------------------------------------------------------------------
model:
  in_dim: 5          # int: input dimension
  out_dim: 5         # int: output dimension
  num_hidden: 3      # int: number of hidden layers
  hidden_dim: 50     # int: width of hidden layers
  gamma: 1.0         # float: REQUIRED, must be positive. initialization scaling exponent
                     #   std = hidden_dim^(-gamma/2)
                     #   0.5 = standard, 0.75 = NTK, 1.0 = mean-field, 1.5+ = saddle-to-saddle
  model_seed: 0            # int: random seed for weight initialization

# -----------------------------------------------------------------------------
# Training
# -----------------------------------------------------------------------------
training:
  lr: 0.0001               # float: learning rate
  batch_size: null         # int or null: samples per batch (null = full batch)
  optimizer: SGD           # string: optimizer class name from torch.optim
                           #   SGD, Adam, AdamW, RMSprop, etc.
  optimizer_params: null   # dict or null: additional optimizer kwargs
                           #   e.g., {momentum: 0.9, weight_decay: 0.01}
  criterion: MSELoss       # string: loss class name from torch.nn
                           #   MSELoss, L1Loss, CrossEntropyLoss, etc.
  batch_seed: 0            # int: random seed for batch shuffling
  track_train_loss: false  # bool: track loss on full training set (offline mode only)

# -----------------------------------------------------------------------------
# Data
# -----------------------------------------------------------------------------
data:
  train_samples: 500    # int: number of training samples
  test_samples: 500    # int or null: number of test samples (null = no test set)
  data_seed: 0          # int: random seed for data generation
  online: false         # bool: if true, sample fresh data each batch (infinite data)
                        #   requires batch_size to be set
  noise_std: 0.0        # float: standard deviation of label noise
  params:               # dict: parameters for teacher matrix generation
    matrix: diagonal    # string: matrix type
                        #   "diagonal" - diagonal matrix with entries scale * [1, 2, ..., d]
                        #     requires: scale (float)
                        #   "power_law" - diagonal matrix with entries scale * i^(-alpha)
                        #     requires: scale (float), alpha (float)
                        #   "identity" - identity matrix, requires out_dim == in_dim
    scale: 10.0         # (for diagonal)

# -----------------------------------------------------------------------------
# Training Loop
# -----------------------------------------------------------------------------
max_steps: 10000        # int: total training steps
num_evaluations: 250    # int: number of times to evaluate/log metrics
                        #   evaluate_every = max_steps // num_evaluations

# -----------------------------------------------------------------------------
# Metrics
# -----------------------------------------------------------------------------
# Metrics are computed on test_set at each evaluation step.
# Two formats:
#   - string: "metric_name" (uses defaults)
#   - dict: {metric_name: {param: value}} (with custom params)
#
# Available metrics:
#   weight_norm               - L2 norm of all parameters
#   layer_norms               - returns layer_norm_0, ... (per-layer Frobenius norms)
#   gram_norms                - returns gram_norm_0, ... (per-layer ||W W^T||_F)
#   balance_diffs             - returns balance_diff_0, ... (per-pair ||W_i W_i^T - W_{i+1}^T W_{i+1}||_F)
#   end_to_end_weight_norm     - Frobenius norm of collapsed end-to-end matrix
#   grad_norm_squared         - ||∇L||², single backward pass
#   trace_gradient_covariance - Tr(Σ), per-sample gradient covariance trace
#                               params: chunks (int, default 1)
#   gradient_stats            - returns grad_norm_squared + trace_gradient_covariance
#                               (shared computation, saves a backward pass vs separate)
#                               params: chunks (int, default 1)
#   trace_hessian_covariance  - Tr(HΣ), Hessian-gradient covariance trace
#                               params: chunks (int, default 1)
#   trace_covariances         - returns all three: grad_norm_squared,
#                               trace_gradient_covariance, trace_hessian_covariance
#                               params: chunks (int, default 1)
#   relative_rank             - rank with user-specified tolerance
#                               params: tol (float, default 0.01), abs_tol (float, default 0.0)
#                               threshold = max(tol * sigma_max, abs_tol)
#                               set abs_tol > 0 so near-zero matrices get rank 0
#   singular_values           - returns sv_0, sv_1, ... (all singular values of effective weight)
#   layer_singular_values     - returns layer_0_sv_0, ... (per-layer singular values)
#   partial_product_singular_values - returns pp_0_0_sv, pp_0_1_sv, ...
#                               (singular value vectors for all partial products P(i,j) = W_j...W_i)
#   partial_product_rank_metrics    - returns pp_0_0_relative_rank, pp_0_1_relative_rank, ...
#                               params: tol (float, default 0.01), abs_tol (float, default 0.0)
#   partial_product_metrics   - returns both SVs and rank for all partial products
#                               from a single SVD per product (preferred bundle metric)
#                               params: tol (float, default 0.01), abs_tol (float, default 0.0)
metrics: []
# Examples:
#   metrics:
#     - weight_norm
#     - trace_covariances:
#         chunks: 10
#     - relative_rank:
#         tol: 0.05


# -----------------------------------------------------------------------------
# Callbacks
# -----------------------------------------------------------------------------
# Callbacks are called at each training step.
# Format: {callback_name: {param: value}}
#
# Available callbacks:
#   switch_batch_size       - change batch size at a specific step
#                             params: at_step (int), batch_size (int or null)
#   multi_switch_batch_size - change batch size at multiple steps
#                             params: schedule (dict mapping step -> batch_size)
#   lr_decay                - multiply learning rate by factor every N steps
#                             params: decay_every (int), factor (float)
callbacks: []
# Examples:
#   callbacks:
#     - switch_batch_size:
#         at_step: 1000
#         batch_size: null
#     - lr_decay:
#         decay_every: 5000
#         factor: 0.5