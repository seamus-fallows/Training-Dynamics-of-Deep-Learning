defaults:
  - paths
  - _self_

experiment:
  name: "diagonal_teacher_comparative"

# SHARED VALUES
# Change these to update both models/trainers simultaneously.
shared:
  model:
    in_dim: 5
    out_dim: 5
    num_hidden: 3
    hidden_dim: 10
    gamma: 1.5
    bias: false
  
  training:
    lr: 0.0005
    batch_size: null
    optimizer: "SGD"
    optimizer_params: null
    criterion: "MSELoss"
    model_seed: 0


# INDIVIDUAL MODEL CONFIGS
# Hydra interpolation (${shared...}) keeps both models synced by default.

model_a:
  in_dim: ${shared.model.in_dim}
  out_dim: ${shared.model.out_dim}
  num_hidden: ${shared.model.num_hidden}
  hidden_dim: ${shared.model.hidden_dim}
  gamma: ${shared.model.gamma}
  bias: ${shared.model.bias}

model_b:
  in_dim: ${shared.model.in_dim}
  out_dim: ${shared.model.out_dim}
  num_hidden: ${shared.model.num_hidden}
  hidden_dim: ${shared.model.hidden_dim}
  gamma: ${shared.model.gamma}
  bias: ${shared.model.bias}


# INDIVIDUAL TRAINING CONFIGS

training_a:
  lr: ${shared.training.lr}
  batch_size: ${shared.training.batch_size}
  optimizer: ${shared.training.optimizer}
  optimizer_params: ${shared.training.optimizer_params}
  criterion: ${shared.training.criterion}
  model_seed: ${shared.training.model_seed}

training_b:
  lr: ${shared.training.lr}
  batch_size: ${shared.training.batch_size}
  optimizer: ${shared.training.optimizer}
  optimizer_params: ${shared.training.optimizer_params}
  criterion: ${shared.training.criterion}
  model_seed: ${shared.training.model_seed}


data:
  type: "linear_teacher"
  num_samples: 100
  test_split: null
  data_seed: 0
  online: false
  noise_std: 0.0
  params:
    matrix: "diagonal"
    scale: 10.0

max_steps: 2000
evaluate_every: 1
model_metrics: []
comparative_metrics:        
  - "param_distance"

callbacks_a: []
callbacks_b: []


observables: null
  # names: ["golden_path_stats"]
  # evaluate_every: 1
  # mode: "population"
  # holdout_size: null